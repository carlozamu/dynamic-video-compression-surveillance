{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 4 PT. 2: MOTION ESTIMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To learn about the various functions and how to call them, please type the command: \u001b[94m python3 main.py --help \u001b[0m \n",
      "\n",
      "The program will perform a Three Step Search and then ask if you want to see the Optical Flow \n",
      "\n",
      "\u001b[93m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ThreeStepSearch with Normal accuracy:   1%|‚ñè         | 5/381 [00:00<00:36, 10.36it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTo learn about the various functions and how to call them, please type the command: \u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[94m python3 main.py --help \u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe program will perform a Three Step Search and then ask if you want to see the Optical Flow \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mblockMatching\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideopath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearchArea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearchType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThreeStepSearch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrore_enable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_Accuracy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNormal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_Vector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# blockMatching.main(videopath, outpath, blockSize, searchArea, searchType=\"ExhaustiveSearch\", errore_enable=True, pixel_Accuracy=\"Normal\", plot_Vector=False)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# blockMatching.main(videopath, outpath, blockSize, searchArea, searchType=\"2DLogSearch\", errore_enable=True, pixel_Accuracy=\"Normal\", plot_Vector=False)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m seeOptical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWould you like to see the live opticalFlow? (True/False)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/LAB4_SOL/VideoMotion/blockMatching.py:531\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(videopath, outpath, blockSize, searchArea, searchType, errore_enable, pixel_Accuracy, plot_Vector)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m#Y,Cr,Cb - channel level, Y - reconstructed\u001b[39;00m\n\u001b[0;32m--> 531\u001b[0m frame_YCrCb, processed_Y \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpixel_Accuracy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m#First frame is the Intra Frame, then every 16 frame we have a new Intra Frame to recover the error\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m#codec_Memory contains the frame of the previous iteration\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_frame \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m num_frame\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m15\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/LAB4_SOL/VideoMotion/blockMatching.py:433\u001b[0m, in \u001b[0;36mpreprocessing\u001b[0;34m(frame, pixel_Accuracy)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03mFrame preparation for processing. Frame conversion and decomposition in YCrCb\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03mReturn - frame_YCrCb, processed_Y (Channel Y, processed with DCT, IDCT and Quantization Matrix)\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m frame_YCrCb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2YCrCb)\n\u001b[0;32m--> 433\u001b[0m processed_Y \u001b[38;5;241m=\u001b[39m \u001b[43mquantization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_YCrCb\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_Accuracy \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixel_Accuracy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHalf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/LAB4_SOL/VideoMotion/blockMatching.py:148\u001b[0m, in \u001b[0;36mquantization\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m#Apply IDCT and Quantization_Matrix to 8x8 blocks\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mr_[:size[\u001b[38;5;241m0\u001b[39m]:\u001b[38;5;241m8\u001b[39m]:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mr_[:size[\u001b[38;5;241m1\u001b[39m]:\u001b[38;5;241m8\u001b[39m]:\n\u001b[1;32m    149\u001b[0m         block \u001b[38;5;241m=\u001b[39m frame_dct[i:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m8\u001b[39m),j:(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m8\u001b[39m)]\n\u001b[1;32m    150\u001b[0m         frame_dct[i:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m8\u001b[39m),j:(j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m8\u001b[39m)] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maround(idct2(block\u001b[38;5;241m*\u001b[39mquantization_matrix))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import VideoMotion.blockMatching as blockMatching\n",
    "import VideoMotion.opticalFlow as opticalFlow\n",
    "\n",
    "if len(sys.argv)==2 and sys.argv[1]=='--help':\n",
    "    help(blockMatching)\n",
    "    help(opticalFlow)\n",
    "    exit()\n",
    "\n",
    "videopath = \"VideoMotion/Input/burglar3.mp4\"\n",
    "outpath = \"Output\"\n",
    "blockSize = 16\n",
    "searchArea = 7\n",
    "\n",
    "print(\"\\nTo learn about the various functions and how to call them, please type the command: \\033[94m python3 main.py --help \\033[0m \\n\")\n",
    "print(\"The program will perform a Three Step Search and then ask if you want to see the Optical Flow \\n\")\n",
    "\n",
    "blockMatching.main(videopath, outpath, blockSize, searchArea, searchType=\"ThreeStepSearch\", errore_enable=True, pixel_Accuracy=\"Normal\", plot_Vector=True)\n",
    "# blockMatching.main(videopath, outpath, blockSize, searchArea, searchType=\"ExhaustiveSearch\", errore_enable=True, pixel_Accuracy=\"Normal\", plot_Vector=False)\n",
    "# blockMatching.main(videopath, outpath, blockSize, searchArea, searchType=\"2DLogSearch\", errore_enable=True, pixel_Accuracy=\"Normal\", plot_Vector=False)\n",
    "\n",
    "seeOptical = input(\"\\nWould you like to see the live opticalFlow? (True/False)\\n\")\n",
    "if seeOptical == \"True\":\n",
    "    opticalFlow.main(videopath, outpath, blockSize, displayHeight=480, show=True, save=True, live=False, dense=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Basic Optical Flow Computation\n",
    "\n",
    "Choose a video file and load it.\n",
    "Implement an optical flow algorithm (e.g., Lucas-Kanade or Farneback method) to compute the optical flow between consecutive frames.\n",
    "Visualize the computed optical flow using arrows or color-coded representations.\n",
    "Experiment with different parameters of the optical flow algorithm and observe the effects on the computed flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute optical flow\n",
    "def compute_optical_flow(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    ret, first_frame = cap.read()\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create a mask image for drawing purposes\n",
    "    mask = np.zeros_like(first_frame)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the current frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute optical flow using the Lucas-Kanade method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # Compute the magnitude and angle of the optical flow vectors\n",
    "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        # Set the hue value according to the optical flow direction\n",
    "        mask[..., 0] = angle * 180 / np.pi / 2\n",
    "\n",
    "        # Set the value of the saturation to the magnitude of the optical flow\n",
    "        mask[..., 1] = 255\n",
    "\n",
    "        # Normalize the magnitude to the range [0, 255]\n",
    "        mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # Convert the HSV image to BGR for display\n",
    "        optical_flow = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        # Display the original frame and the optical flow side by side\n",
    "        cv2.imshow('Optical Flow', np.hstack([frame, optical_flow]))\n",
    "\n",
    "        # Update the previous frame\n",
    "        prev_gray = gray\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Specify the path to the video file\n",
    "video_path = 'VideoMotion/Input/mobile_cif.mov'\n",
    "\n",
    "# Call the function to compute and display optical flow\n",
    "compute_optical_flow(video_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Real-time Optical Flow\n",
    "\n",
    "Modify the previous exercise to perform real-time optical flow estimation from a webcam stream or video.\n",
    "Display the original video frames alongside the optical flow visualization in real-time.\n",
    "Implement an interface that allows users to pause, resume, and adjust the speed of the video stream while visualizing optical flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 15:53:10.798 Python[57044:2565113] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-19 15:53:10.798 Python[57044:2565113] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Call the function to compute and display optical flow from webcam input\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[43mcompute_optical_flow_webcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mcompute_optical_flow_webcam\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Compute optical flow using the Lucas-Kanade method\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m flow \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalcOpticalFlowFarneback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_gray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Compute the magnitude and angle of the optical flow vectors\u001b[39;00m\n\u001b[1;32m     31\u001b[0m magnitude, angle \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcartToPolar(flow[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m], flow[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to compute optical flow from webcam input\n",
    "def compute_optical_flow_webcam():\n",
    "    cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (you may need to adjust it)\n",
    "\n",
    "    # Check if the webcam is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    ret, first_frame = cap.read()\n",
    "    prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create a mask image for drawing purposes\n",
    "    mask = np.zeros_like(first_frame)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the current frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute optical flow using the Lucas-Kanade method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "        # Compute the magnitude and angle of the optical flow vectors\n",
    "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "\n",
    "        # Set the hue value according to the optical flow direction\n",
    "        mask[..., 0] = angle * 180 / np.pi / 2\n",
    "\n",
    "        # Set the value of the saturation to the magnitude of the optical flow\n",
    "        mask[..., 1] = 255\n",
    "\n",
    "        # Normalize the magnitude to the range [0, 255]\n",
    "        mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "        # Convert the HSV image to BGR for display\n",
    "        optical_flow = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        # Display the original frame and the optical flow side by side\n",
    "        cv2.imshow('Optical Flow', np.hstack([frame, optical_flow]))\n",
    "\n",
    "        # Update the previous frame\n",
    "        prev_gray = gray\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam capture object and close all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Call the function to compute and display optical flow from webcam input\n",
    "compute_optical_flow_webcam()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_lab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
